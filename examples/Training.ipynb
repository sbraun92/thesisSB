{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exemplary Training: Q-Learning, SARSA and DQN\n",
    "\n",
    "\n",
    "\n",
    "This notebook aims to demonstrate the training process of an agent in practise. For simplicity only the stowage planning problem for a small RORO deck is shown with the loading list outlined in the corresponding thesis (for algorithms default values have been defined which may be found in the annex of the thesis)\n",
    "\n",
    "On the other hand the notebook `Example.ipynb` shows  how an already trained model could be used in practise.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Imports\n",
    "\n",
    "Firstly various modules are imported (including agent classes, environment classes, a plotting unit and a logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath('../'))\n",
    "\n",
    "from analysis.plotter import Plotter\n",
    "from analysis.evaluator import *\n",
    "from analysis.loggingUnit import LoggingBase\n",
    "from env.roroDeck import RoRoDeck\n",
    "from agent import sarsa, tdq, dqn\n",
    "from analysis.algorithms import *\n",
    "\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Register logger, choose algorithm type and set training duration\n",
    "\n",
    "The first step is to register the logging unit. This will also set the output path where the trained models will be saved. Furthermore it has to be decided on how many iterations the agent should be trained. If this is not set the agent will fall back on default values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register Output path and Logger\n",
    "loggingBase = LoggingBase()\n",
    "module_path = loggingBase.module_path\n",
    "\n",
    "print('Training outputs will be save to:\\n'+str(module_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose algorithm by changing the value of `algorithm_type`:\n",
    "\n",
    "    - SARSA            0\n",
    "    - Q-Learning       1\n",
    "    - DQ-Learning      2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm_type = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithms = ['SARSA','TDQ','DQN']\n",
    "algorithm = algorithms[algorithm_type]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose number of training episodes. Default values are:\n",
    "    \n",
    "    - SARSA          600_000\n",
    "    - Q-Learning     600_000\n",
    "    - DQ-Learning     14_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_episodes = 6_000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Initalise the environment\n",
    "\n",
    "Secondly the environment is initialised. It can be decided the size of the environment and if it should behave stochasticly. If `stochastic` is set to true than the agent will behave with probability $p$ (`env.p`) determinisitic in a sense that the cargo type chosen by the agent is actually loaded. Subsequently, a random cargo type is loaded with probability $1-p$.\n",
    "\n",
    "**Note:** In the thesis the environment is said to be deterministic. Since deviations are not assumed to happen regularly.\n",
    "\n",
    "The `vehicle_data` variable corresponds to the loading list which may be changed by the user. After every change the environment needs to be reset.\n",
    "\n",
    "The `reset()`-method will return the representation of the intial state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = np.array([[ 0,  1,  2,  3,  4],\n",
    "       [ 5,  5, -1, -1,  2],\n",
    "       [ 1,  1,  0,  0,  1],\n",
    "       [ 1,  2,  1,  2,  2],\n",
    "       [ 3,  4,  2,  3,  2],\n",
    "       [ 0,  0,  0,  0,  1]])\n",
    "\n",
    "l2 = np.array([[ 0,  1,  2,  3,  4,  5,  6],\n",
    "       [ 5,  5, -1, -1,  2,  2,  2],\n",
    "       [ 1,  1,  0,  0,  1,  1,  1],\n",
    "       [ 1,  2,  1,  2,  2,  1,  2],\n",
    "       [ 2,  3,  2,  3,  2,  2,  3],\n",
    "       [ 0,  0,  0,  0,  1,  0,  0]])\n",
    "\n",
    "\n",
    "\n",
    "env = RoRoDeck(lanes=8, rows=12)\n",
    "\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Train the agent\n",
    "\n",
    "Train the agent with the environment.\n",
    "The user may choose between different algorithms:\n",
    "- TDQ-Learning\n",
    "- SARSA\n",
    "- Deep Q-Learning (DQN)\n",
    "\n",
    "The training is started by calling `agent.train()`. After the last training episode the `train()`-method will show a grid representation of the final stowage plan.\n",
    "\n",
    "**Important Note:** The run time of this method might depend on how much memory is already used for Jupyter-Notebooks and on the browser settings. The `main()`-methods of `tdq.py`,`sarsa.py` and `dqn.py` are demonstrating the usage equivalently and might run faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# If DQN is used the number of episodes should not exceed roughly 14_000 (default value)\n",
    "# to solve the problem in reasonable time if GPU cannot be used for training\n",
    "\n",
    "if algorithm is 'DQN':\n",
    "    assert 10_000 <=number_of_episodes <= 14_000\n",
    "\n",
    "\n",
    "print('Train agent with '+algorithm+'\\n')\n",
    "\n",
    "if algorithm == 'SARSA':\n",
    "    agent = sarsa.SARSA(env, module_path, number_of_episodes)\n",
    "elif algorithm == 'TDQ':\n",
    "    agent = tdq.TDQLearning(env, module_path, number_of_episodes)\n",
    "else:\n",
    "    agent = dqn.DQLearningAgent(env=env, module_path=module_path, number_of_episodes=number_of_episodes,\n",
    "                                layers= [128,128])\n",
    "    \n",
    "# Call train-method\n",
    "model, total_rewards, vehicle_loaded, eps_history, state_expansion = agent.train()\n",
    "# Save model to output path\n",
    "agent.save_model(module_path)\n",
    "\n",
    "print(agent.get_info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Plot training performance\n",
    "\n",
    "This will plot:\n",
    "1. Reward over time\n",
    "2. The size of the Q-table if this was a tabular method (State Expansion).\n",
    "3. The steps to finish (also how may vehicles are loaded to the deck)\n",
    "4. The $\\epsilon$-development over time for $\\epsilon$-greedy exploration.\n",
    "\n",
    "**Note:** The smothing window will smooth the ouput plots to make trends more visible. It will take the average of the last $n$ iterations where $n$ corresponds to the number defined by the variable `smoothing_window`. This is highly recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter = Plotter(module_path, number_of_episodes, algorithm=algorithm,\n",
    "                  smoothing_window=200, show_plot=True, show_title=True)\n",
    "plotter.plot(total_rewards, state_expansion, vehicle_loaded, eps_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Evaluation\n",
    "\n",
    "Evaluate the final stowage plan after execution always the best action.\n",
    "\n",
    "An optimal stowage plan would result in:\n",
    "\n",
    "> Mandatory Cargo Loaded: 1.0\n",
    "\n",
    "This stowage plan will load 100% of the mandatory cargo and ...\n",
    "\n",
    "> Number of Shifts: 0.0\n",
    "\n",
    "... causes zero shifts.\n",
    "\n",
    "> Space Utilisation: 1.0\n",
    "\n",
    "Moreover the space would be used to 100%.\n",
    "\n",
    "\n",
    "The final stowage plan created by the agent evaluates as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Evaluator(env.vehicle_data, env.grid)\n",
    "evaluation = evaluator.evaluate(env.get_stowage_plan())\n",
    "print(evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics, info = training_metrics(total_rewards)\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
