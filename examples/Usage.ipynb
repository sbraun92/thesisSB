{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Usage of RL for Stowage Planning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Firstly various modules are imported (including agent classes, environment classes, a plotting unit and a logger)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.insert(0, os.path.abspath('../'))\n",
    "module_path = str(os.getcwd())+str(Path('/out/'))\n",
    "\n",
    "from env import roroDeck\n",
    "from analysis import *\n",
    "from analysis.algorithms import *\n",
    "from analysis.evaluator import Evaluator\n",
    "from analysis.loggingUnit import LoggingBase\n",
    "\n",
    "\n",
    "from agent.tdq import TDQLearning\n",
    "from agent.sarsa import SARSA\n",
    "from agent.dqn import DQLearningAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a model and show input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set paths to trained models\n",
    "By using the path by `LoggingBase(prepare_training = False).module_path` the path is set to the location where RL-systems are stored when using `Training.iynb`. The actual path and file name needs to added. This depends on date and time of production, algorithm type and problem set.\n",
    "\n",
    "The following information needs to be added in the following form:\n",
    "\n",
    "`\"/YYYYMMDD/hhmm/hhmmAlgorithm_LX_RX.type\"`\n",
    "\n",
    "For example:\n",
    "\n",
    "`\"/20201118/1927/1927SARSA_L8-R12.p\"`\n",
    "RL-system was produced on 18.11.2020 19:27 trained with SARSA for 8 Lanes and 12 Rows.\n",
    "All trained models are in the folder:\n",
    "\n",
    "    .../thesisSB/analysis/out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify if own trained models should be tested\n",
    "\n",
    "path_string_1 = \"20201118/1927/1927SARSA_L8-R14.p\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Or** specify the path to trained prototypes. For example:\n",
    "\n",
    "`\"DQLearning/L08-R14-L0/DQLearning_L08-R14-L0.h5\"`\n",
    "\n",
    "\n",
    "**Note: It is assumed that the folder `output` is on the same level as the folder `thesisSB`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify to test submitted prototypes\n",
    "path_string_2 = \"SARSA/L08-R14-L0/SARSA_L08-R14-L0.p\"\n",
    "\n",
    "#path_string_2 = \"DQLearning/L08-R14-L0/DQLearning_L08-R14-L0.h5\"\n",
    "#path_string_2 = \"QLearning/L08-R14-L0/TDQ_L08-R14-L0.p\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose if `path_string_2` (submitted prototypes) or `path_string_1` (own trained models with `Training.iynb`) is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_path_string_2 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = LoggingBase(prepare_training = False).module_path\n",
    "if use_path_string_2:\n",
    "    path = path.parents[2].joinpath('output/').joinpath(path_string_2)\n",
    "else:\n",
    "    path = path.joinpath(path_string_1)\n",
    "\n",
    "print('Will load model saved at:\\n'+str(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set environment according to input data\n",
    "\n",
    "The environment input data must fit to the trained model (number of lanes and rows, loading list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pass loading list by setting RoRo-deck environment th argument vehicle_data\n",
    "# For example: roroDeck.RoRoDeck(vehicle_data=loading_list_1)\n",
    "\n",
    "loading_list_1 = np.array([[ 0,  1,  2,  3,  4,  5,  6],\n",
    "                           [ 5,  5, -1, -1,  2,  2,  2],\n",
    "                           [ 1,  1,  0,  0,  1,  1,  1],\n",
    "                           [ 1,  2,  1,  2,  2,  1,  2],\n",
    "                           [ 2,  3,  2,  3,  2,  2,  3],\n",
    "                           [ 0,  0,  0,  0,  1,  0,  0]])\n",
    "\n",
    "loading_list_2 = np.array([[0,   1,  2,  3,  4], \n",
    "                           [5,   5, -1, -1,  2], \n",
    "                           [1,   1,  0,  0,  1], \n",
    "                           [1,   2,  1,  2,  2], \n",
    "                           [3,   4,  2,  3,  2], \n",
    "                           [0,   0,  0,  0,  1]]) \n",
    "\n",
    "\n",
    "# Environment dimensions must fit to the prototype\n",
    "env = roroDeck.RoRoDeck(lanes=8, rows=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create agent, bind environment and load model\n",
    "\n",
    "**Specify the algorithm type according to the trained model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#agent = TDQLearning(env,path)\n",
    "agent = SARSA(env,path)\n",
    "#agent = DQLearningAgent(env,path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add specific path\n",
    "agent.load_model(path)\n",
    "\n",
    "env = agent.env\n",
    "evaluator = Evaluator(env.vehicle_data, env.grid)\n",
    "\n",
    "if not path.exists():\n",
    "    print('No such file. Check path!') \n",
    "else:\n",
    "    if path.suffix == \".p\":   #Check if it is a pickle file\n",
    "        for info in agent.q_table[\"ModelParam\"]:\n",
    "            print(info +': '+ str(agent.q_table[\"ModelParam\"][info]))\n",
    "    else:\n",
    "        print(agent.info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show best stowage plan the agent found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "agent.execute(env)\n",
    "evaluation = evaluator.evaluate(env.get_stowage_plan())\n",
    "env.render()\n",
    "print(evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the loading sequence which constructed this stowage plan "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.loading_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human Interface\n",
    "\n",
    "#### Construct a stowage plan sequentially\n",
    "\n",
    "Example of a an human-agent interaction:\n",
    "The agent shows the predictions for each action in a given state and recommends the best one.\n",
    "If the environment is set to deterministic behaviour the stowage plan above is reconstructed if all recommendations are obeyed\n",
    "\n",
    "Usage (Type the following on the Keyboard and press Enter):\n",
    "\n",
    "0,1,2 &nbsp; &nbsp; &nbsp; &nbsp; *number from the list of possible actions*\n",
    "\n",
    "r &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; *show the current state of the RORO-deck*\n",
    "\n",
    "b &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;*follow the best action*\n",
    "\n",
    "e &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;*stop interaction by only following the recommendations of agent*\n",
    "\n",
    "q &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;*quit the execution*\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "done = False\n",
    "counter = 1\n",
    "while not done:\n",
    "    print(\"\\n\"+str(counter)+\". Vehicle\\nPossible Actions for Lane \"+str(env.current_Lane)\n",
    "          +\"\\n\"+str(env.possible_actions))\n",
    "    source = []\n",
    "    for action in env.possible_actions:\n",
    "        source += [round(float(agent.predict(state,action)),2)]\n",
    "    best_action = agent.max_action(state,env.possible_actions)\n",
    "    \n",
    "    #print(env.possible_actions)\n",
    "    #source = DQN_agent.q_eval.predict(state[np.newaxis, :])\n",
    "    #best_action = DQN_agent.maxAction(state,env.possible_actions)\n",
    "    #print(\"Prediction of Agent:\\n\"+str(source[0][env.possible_actions]))\n",
    "    print(\"Prediction of Agent:\\n\"+str(source))\n",
    "    print(\"--> Choose: \"+str(best_action))\n",
    "    print(\"\\n(or 'r' for render \\t 'e' for execute \\t 'b' for best action \\t 'q' for quitting the program)\")\n",
    "    \n",
    "    action = input() \n",
    "    try:\n",
    "        action = int(action)\n",
    "        if int(action) in env.possible_actions:\n",
    "            state, reward, done, info = env.step(int(action))\n",
    "            counter += 1            \n",
    "    except:\n",
    "        if action == 'b':\n",
    "            state, reward, done, info = env.step(int(best_action))\n",
    "            counter += 1\n",
    "        elif action == 'e':\n",
    "            #DQN_agent.execute(env)\n",
    "            agent.execute(env)\n",
    "            #agent.execute()\n",
    "            break\n",
    "        elif action == 'q':\n",
    "            print(\"Quit interaction mode\")\n",
    "            break\n",
    "        elif action == 'r':\n",
    "            env.render()\n",
    "env.render()\n",
    "print(\"\\n\\n\")\n",
    "evaluation = evaluator.evaluate(env.get_stowage_plan())\n",
    "print(evaluation)\n",
    "print(\"\\n\\n\")\n",
    "print(env.loading_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
